# Локальный STT и диаризация спикеров для русского языка

Единой модели speech-to-text со встроенной диаризацией спикеров на сегодня не существует — все production-решения комбинируют отдельные ASR и diarization модели в пайплайнах. Для русского языка наиболее практичный подход — связка **Whisper large-v3** (или fine-tuned вариантов с **WER 6.4%**) с **pyannote-audio 3.1** для диаризации, поскольку модели спикерных эмбеддингов принципиально language-agnostic. NVIDIA NeMo предлагает альтернативный стек с выделенными русскоязычными ASR-моделями, обученными на 1800+ часах русской речи.

Ключевой вывод: точность диаризации зависит больше от качества аудио и перекрытия речи спикеров, чем от языка. Diarization Error Rate (DER) варьируется от **7% до 20%** в зависимости от сложности датасета, причём основной источник ошибок — пропущенная речь (missed speech). Real-time обработка достижима с латентностью **500мс–5с** через специализированные streaming-реализации типа diart или NVIDIA Streaming Sortformer.

---

## Комбинированные решения STT + диаризация

**WhisperX** — самое популярное интегрированное решение (18.7k звёзд на GitHub, активно поддерживается). Связывает faster-whisper (70x realtime скорость транскрипции) с wav2vec2 forced alignment и pyannote-audio диаризацией. Пайплайн обрабатывает аудио в четыре стадии: Whisper транскрипция → wav2vec2 forced alignment → pyannote speaker diarization → присвоение спикеров словам. Для русского нужно вручную указать модель выравнивания (`jonatasgrosman/wav2vec2-large-xlsr-53-russian`), так как русский не включён в дефолтные модели.

| Решение | Бэкенд диаризации | Русский STT | GPU VRAM | Лицензия |
|---------|-------------------|-------------|----------|----------|
| WhisperX | pyannote-audio 3.1 | ✅ Нативно | <8GB (large-v2) | BSD-2 |
| whisper-diarization | NeMo TitaNet | ✅ Нативно | ≥10GB параллельно | BSD-2 |
| insanely-fast-whisper | pyannote-audio 3.1 | ✅ Нативно | Варьируется | MIT |
| NeMo MSDD pipeline | TitaNet + MSDD | ✅ Выделенные модели | ≥10GB | Apache 2.0 |

**whisper-diarization** (MahmoudAshraf97) предлагает альтернативную архитектуру на стеке NVIDIA: MarbleNet для VAD, TitaNet для спикерных эмбеддингов и Facebook Demucs для предварительного выделения вокала. Этот шаг улучшает точность диаризации на зашумлённых записях. Требует ≥10GB VRAM для параллельного режима, русский поддерживается через `--language ru`.

**NVIDIA NeMo** предоставляет наиболее близкое к end-to-end решение через модель **Sortformer** — transformer-энкодер, генерирующий метки спикеров напрямую из сырого аудио без отдельного извлечения эмбеддингов. Однако Sortformer — только диаризация (не ASR) и ограничен 4 спикерами. Для production каскадный пайплайн NeMo с Multi-Scale Diarization Decoder (MSDD) остаётся более зрелым и поддерживает произвольное число спикеров через pairwise inference.

---

## Standalone-инструменты диаризации

**pyannote-audio 3.1** — де-факто стандарт для speaker diarization, используется как бэкенд в WhisperX и insanely-fast-whisper. Последняя версия — чистый PyTorch (убрали зависимость от ONNX) и достигает **2.5% real-time factor** на NVIDIA V100 — час аудио обрабатывается примерно за 90 секунд. Доступ требует HuggingFace токен и принятия лицензии для gated-моделей.

```python
from pyannote.audio import Pipeline
import torch

pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization-3.1", 
                                     use_auth_token="HF_TOKEN")
pipeline.to(torch.device("cuda"))

diarization = pipeline("audio.wav", min_speakers=2, max_speakers=5)

for turn, _, speaker in diarization.itertracks(yield_label=True):
    print(f"{turn.start:.1f}s - {turn.end:.1f}s: {speaker}")
```

**NVIDIA NeMo TitaNet** предоставляет 23M-параметровые спикерные эмбеддинги, превосходящие старые x-vector подходы. В сочетании с MarbleNet VAD и MSDD neural diarizer этот стек лучше справляется с перекрывающейся речью, чем методы только на кластеризации. Multi-Scale Diarization Decoder извлекает эмбеддинги из сегментов разной длины и динамически взвешивает каждую шкалу, решая фундаментальный tradeoff между временным разрешением и качеством представления спикера.

Для лёгкого деплоя **Resemblyzer** предлагает 256-мерные эмбеддинги со скоростью ~1000x realtime на consumer GPU. Это building blocks, а не готовый пайплайн — кластеризацию реализуете сами. **simple-diarizer** оборачивает SpeechBrain ECAPA-TDNN эмбеддинги с Silero VAD и spectral clustering для drop-in решения с минимальной конфигурацией.

**WeSpeaker** от команды WeNet связывает исследования и production с моделями в форматах PyTorch и ONNX. Использует UMAP для снижения размерности с HDBSCAN кластеризацией и предоставляет CLI для быстрого деплоя без написания Python-кода.

---

## Специфика русского языка

Speaker diarization принципиально **language-agnostic** на уровне эмбеддингов — модели извлекают акустические признаки (pitch, тембр, темп речи), а не фонетическое содержание. Модели pyannote, NeMo TitaNet и ECAPA-TDNN, обученные на VoxCeleb, работают на разных языках без дообучения. Документация NeMo подтверждает тестирование на данных митингов на мандаринском и 4-спикерном CALLHOME non-English set.

Для русского ASR есть несколько качественных вариантов:

| Модель | WER | Данные обучения | Примечания |
|--------|-----|-----------------|------------|
| Whisper large-v3 | ~9.8% | Мультиязычные | Baseline |
| antony66/whisper-large-v3-russian | **6.4%** | Fine-tuned | Лучший open-source |
| nvidia/stt_ru_fastconformer_hybrid_large_pc | Конкурентный | 1840 часов русского | Выделенная модель NeMo |
| Vosk Russian models | Варьируется | Несколько | Offline-first, малый размер |

Русские речевые корпуса для обучения: **Golos** (1240 часов от SberDevices), **Open STT** (~16M высказываний) и **Russian LibriSpeech**. Они позволяют fine-tuning как ASR, так и потенциально компонентов диаризации, хотя русскоспецифичных моделей диаризации в публичных репозиториях пока нет.

Основное ограничение — **модели forced alignment** для word-level timestamps. WhisperX по умолчанию использует английские/французские/немецкие/испанские/итальянские wav2vec2 модели; для русского нужно вручную загружать `jonatasgrosman/wav2vec2-large-xlsr-53-russian` или `jonatasgrosman/wav2vec2-xls-r-1b-russian` с HuggingFace.

VAD-модели, обученные преимущественно на английском, могут работать чуть хуже на русском. Рассмотрите fine-tuning Silero VAD или MarbleNet на русской речи при проблемах с точностью.

---

## Real-time диаризация

Batch и streaming диаризация принципиально различаются архитектурно. Batch анализирует целые записи с глобальной кластеризацией и двунаправленным контекстом. Streaming должен обрабатывать аудио инкрементально с ограниченным будущим контекстом, поддерживая консистентные ID спикеров между чанками и детектируя новых спикеров в реальном времени.

**diart** — референсная реализация для streaming pyannote-based диаризации. Использует скользящий буфер с обновлением каждые 500мс и инкрементальной кластеризацией, достигая настраиваемой латентности между **500мс и 5с**. Tradeoff прямой: ниже латентность — меньше контекста для решений о спикерах.

| Настройка латентности | Точность | Применение |
|----------------------|----------|------------|
| 500мс | Ниже | Live captioning |
| 1-2с | Баланс | Интерактивные приложения |
| 5с | Высшая | Near-real-time транскрипция |

**NVIDIA Streaming Sortformer** (`nvidia/diar_streaming_sortformer_4spk-v2`) — production-grade вариант для real-time диаризации в экосистеме NeMo/Riva. Использует Arrival-Order Speaker Cache (AOSC) для отслеживания спикеров между фреймами, сохраняя frame-level эмбеддинги с наивысшими confidence scores. Этот подход решает speaker permutation — ключевую проблему при обработке аудио чанками.

```python
# diart streaming пример
from diart import SpeakerDiarization
from diart.sources import MicrophoneAudioSource
from diart.inference import StreamingInference

pipeline = SpeakerDiarization()
source = MicrophoneAudioSource()
inference = StreamingInference(pipeline, source)
prediction = inference()
```

Batch-обработка остаётся быстрее для постобработки. WhisperX достигает 70x realtime с batched inference, pyannote обрабатывает час за ~90 секунд на V100. Для production-систем, работающих с архивными записями, batch-обработка с параллельными воркерами значительно превышает требования real-time.

---

## Требования к железу

GPU-память — основное ограничение для self-hosted деплоя. Размеры моделей Whisper напрямую определяют требования к VRAM:

| Модель Whisper | Параметры | Требуется VRAM | Фактор скорости |
|----------------|-----------|----------------|-----------------|
| tiny | 39M | ~1 GB | 32x realtime |
| small | 244M | ~2 GB | 6x realtime |
| medium | 769M | ~5 GB | 2x realtime |
| large-v3 | 1.55B | ~10 GB | 1x (baseline) |

**faster-whisper** с CTranslate2 оптимизацией снижает потребление памяти large-v2 до <8GB с beam_size=5, позволяя деплой на consumer RTX 3060/4060. Для комбинированных пайплайнов параллельный режим whisper-diarization требует ≥10GB VRAM, так как транскрипция и диаризация работают одновременно.

CPU-only деплой возможен, но значительно медленнее:
- faster-whisper на 8 vCPU обрабатывает 3-минутное аудио за ~60 секунд
- pyannote диаризация на CPU занимает 14-36 минут для 45-минутного аудио
- Vosk даёт лучший CPU-first опыт с выделенными малыми моделями

Рекомендации по hardware для production:
- **Разработка**: RTX 3060 12GB (~$300) справляется с большинством пайплайнов
- **Production**: RTX 4060 Ti 16GB или A10 24GB для запаса
- **Высокая нагрузка**: Несколько A100 с очередью задач (Celery + Redis)

---

## Бенчмарки точности

Diarization Error Rate (DER) измеряет комбинированные ошибки от missed speech, false alarms и speaker confusion. Меньше — лучше, результаты сильно варьируются в зависимости от сложности датасета:

| Датасет | PyannoteAI (коммерческий) | Pyannote 3.1 | NeMo MSDD | Примечания |
|---------|--------------------------|--------------|-----------|------------|
| AMI (митинги) | ~7.2% | ~11% | ~12% | 16% overlap |
| CALLHOME | ~11% | ~15-18% | Сопоставимо | Телефон |
| VoxConverse | Варьируется | ~15-20% | Выше | Broadcast |
| DIHARD III | ~14.5% | ~19-20% | ~20% | Сложный |

Missed speech доминирует в ошибках всех систем. Участки с перекрывающейся речью (15-20% аудио митингов) значительно увеличивают DER — это остаётся нерешённой проблемой для реальных многоспикерных разговоров.

Архитектура pyannote 3.1 специально адресует overlap через powerset multi-class cross-entropy loss в сегментации, позволяя одновременное детектирование нескольких активных спикеров. MSDD NeMo использует pairwise inference для оценки per-speaker вероятностей независимо, обрабатывая произвольное число спикеров.

---

## Паттерны интеграции

**Паттерн 1: Transcribe-then-diarize** (дефолт WhisperX)
```
Аудио → Whisper → wav2vec2 Alignment → pyannote Diarization → Speaker Assignment
```
Лучше для точности. Forced alignment даёт word-level timestamps, точно маппящиеся на сегменты диаризации. Требует language-specific alignment модели.

**Паттерн 2: Diarize-then-transcribe**
```
Аудио → pyannote Diarization → Разбиение по спикерам → Whisper на сегмент → Merge
```
Чище разделение спикеров и детекция языка per-speaker. Выше overhead от множественных вызовов Whisper.

**Паттерн 3: Параллельная обработка** (NeMo, whisper-diarization)
```
Аудио → [Whisper Transcription] ─┐
      → [NeMo Diarization]     ─┴→ Merge with Alignment
```
Быстрее всего при достаточном железе (≥10GB VRAM). Требует аккуратного выравнивания timestamps между независимыми выходами.

Обязательная препроцессинг включает **Voice Activity Detection** (снижает галлюцинации, позволяет батчинг) и нормализацию аудио до 16kHz mono. Для длинных записей, превышающих GPU-память, разбивайте по границам тишины с 1-5 секундным перекрытием, обрабатывая последовательно с явной очисткой памяти между чанками:

```python
import gc
import torch

# После каждого чанка:
gc.collect()
torch.cuda.empty_cache()
```

---

## Чеклист деплоя для русского языка

Для production-деплоя на русской речи с диаризацией спикеров:

1. **STT**: Fine-tuned Whisper (`antony66/whisper-large-v3-russian`) или NeMo FastConformer Russian модель
2. **Alignment**: `jonatasgrosman/wav2vec2-large-xlsr-53-russian` для word timestamps
3. **Diarization**: pyannote-audio 3.1 (language-agnostic) с GPU-ускорением
4. **VAD**: pyannote VAD или Silero VAD для препроцессинга
5. **Integration**: WhisperX пайплайн с кастомной спецификацией alignment модели

Docker-деплой с официальными faster-whisper образами и GPU passthrough:
```yaml
services:
  transcription:
    image: lscr.io/linuxserver/faster-whisper:gpu
    environment:
      - WHISPER_MODEL=large-v3
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
```

Для pyannote моделей нужен HuggingFace токен — получите на huggingface.co и примите лицензию для `pyannote/speaker-diarization-3.1` и `pyannote/segmentation-3.0`.

---

## Выводы

Self-hosted русский STT с диаризацией спикеров production-ready на текущих open-source инструментах. Стек WhisperX + pyannote даёт лучший баланс точности, скорости и активности поддержки. Ключевые gaps: обработка перекрывающейся речи (ожидайте на 15-20% выше error rate на аудио митингов) и streaming latency (минимум 500мс с tradeoff по точности). Русскоспецифичных моделей диаризации нет, но language-agnostic спикерные эмбеддинги работают адекватно — фокусируйте оптимизацию на русском ASR fine-tuning и alignment моделях, а не на компонентах диаризации. Для максимальной точности комбинируйте fine-tuned Whisper с коммерческим pyannoteAI; для чистого open-source community модели pyannote достигают конкурентных результатов в пределах 4-8% DER от коммерческих предложений.